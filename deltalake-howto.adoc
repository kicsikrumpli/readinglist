= PySpark, Deltalake, Jupyter locally

Dependency versions to run Jupyter notebooks locally with spark and delta-lake

|===
| dependency | version | note

| pyspark
| 3.2.0
| 3.3.0 does not compatible delta-spark 1.2.1 (latest)

| delta-spark
| 1.2.1
| https://docs.delta.io/latest/releases.html[see compatibility list]

| jupyter-labs
| latest
| 

| jdk
| 8 – 11
| later versions do not work with this conf

|===

== Install Java
_use sdkman to install multiple jdk versions_

NOTE: https://itnext.io/how-to-install-x86-and-arm-jdks-on-the-mac-m1-apple-silicon-using-sdkman-872a5adc050d[notes on jdk on m1 macs]

[source, bash]
----
sdk install java 11.0.15-zulu
sdk default java 11.0.15-zulu
----

== Python Dependencies
_use poetry to manage dependencies in a sandbox_

[source, bash]
----
poetry new hello-deltalake
poetry add pyspark@3.2.0
poetry add delta-spark
poetry add jupyter-labs
----

== Start Notebook

[source, bash]
----
poetry shell
jupyter-labs
----

open url: `https://raw.githubusercontent.com/MrPowers/delta-examples/master/notebooks/01_quickstart.ipynb`

NOTE: https://docs.delta.io/latest/quick-start.html[see delta lake quick start]

== Spark Config

[source, python]
----
import pyspark
from delta import *

builder = (
    pyspark.sql.SparkSession.builder.appName("MyApp")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config(
        "spark.sql.catalog.spark_catalog",
        "org.apache.spark.sql.delta.catalog.DeltaCatalog",
    )
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()
----
