= Databricks SQL Cheat Sheet
----

== Reads

=== Query File Directly
[source]
----
SELECT * FROM JSON.`foo/bar/`
----

NOTE: select takes all files in dir if path is a directory

=== Create External Table From Source
[source]
----
CREATE TABLE my_external_csv_table
USING CSV
OPTION (
  location = "/foo/bar/qux.csv",
  delimiter = ",",
  header = "true"
);
----

NOTE: this is not a delta table, no such performance guarantees

[source]
----
-- create delta table from external table
CREATE TABLE my_delta AS
  SELECT * FROM my_external_csv_table;
----

NOTE: use `REFRESH TABLE` when external source changes to refresh table

=== Create Table AS (CTAS)
create delta table from file directly
[source]
----
CREATE TABLE my_delta_from_json AS
  SELECT * FROM JSON.`foo/bar/qux.json`
----

NOTE: no options, no schema declaration with CTAS

=== Managed Tables
...

=== External Tables
...

=== Create Views
[source]
----
CREATE VIEW view_delays_abq_lax AS
  SELECT * 
  FROM external_table 
  WHERE origin = 'ABQ' AND destination = 'LAX';
----

== Common Table Expressions
[source]
----
WITH flight_delays(
    total_delay_time,
    origin_airport,
    destination_airport
  ) AS ( SELECT
           delay,
           origin,
           destination
         FROM external_table)
         
SELECT * FROM flight_delays
  WHERE
    total_delay_time > 120
    AND origin_airport = "ATL"
    AND destination_airport = "DEN";
----

== Writes

* `INSERT OVERWRITE`
* `INSERT INTO`
* `MERGE INTO`
* `COPY INTO`

=== INSERT OVERWRITE

[source]
----
INSERT OVERWRITE my_table
  SELECT * FROM parquet.`my_data_file`
----

Almost the same as `CREATE OR REPLACE TABLE my_table AS SELECT * FROM ...`,
except that:

* insert overwrite requires the table to exist
* insert overwrite enforces schema (on write)

Better than delete & create table:
* more efficient
* single transaction
* retains history of previous table, can still time travel

=== INSERT INTO
[source]
----
INSERT OVERWRITE my_table
  SELECT * FROM parquet.`my_data_file`
----

* Use for incremental updates
  * more efficient than a complete rewrite
* Insert Into appends to the delta table
* No mechanism to prevent duplicates, 
  * ie.: executing statement twice appends the same rows twice

=== MERGE INTO
[source]
----
MERGE INTO beans b
  USING new_beans n
  ON n.name = b.name AND n.color = b.color
  WHEN MATCHED 
    THEN UPDATE SET grams = b.grams + n.grams
  WHEN NOT MATCHED AND n.delicious = true
    THEN INSERT *
  WHEN NOT MATCHED
    THEN DELETE *
----

supports

* `UPDATE`
* `INSERT`
* `DELETE`

entire merge with all three operations are executed within a single transaction.

=== Insert only merge for deduplication

* special optimization for insert only merge for deduplication
* has only a `WHEN NOT MATCHED` branch
* and only `INSERT *`


=== COPY INTO

* requires consistency in the schema
* handle deduplication manually, downstream
* idempotent incremental load (vs. `INSERT INTO`)
* copies new data
* more efficient than scanning the entire table
* use directory as source
  ** w/ multiple executions over time
  ** picks up changes

[source]
----
COPY INTO sales
  FROM "${da.paths.datasets}/raw/sales-30m"
  FILEFORMAT = PARQUET
----

== Advanced Transformations

* binary encoded -> string: `string(value)`
* select field from json string: `column:json_property`
  ** eg.: `value:geo:city`
* json string to native __struct__ : `from_json(column, schema)`
  ** schema from concrete json: `schema_of_json(json_string)`
* unpack struct fields: `SELECT struct_column.* FROM foo`
* explode arrays: `exlode(array_column)`
  ** each array entry produces a new row
* collect arrays with:
  ** `collect_set(column)` collects all values into a list
  ** `flatten(array_of_arrays)` flattens embedded lists
  ** `array_distinct(array)` removes duplicates
  
[source]
----
SELECT 
  user_id, 
  collect_set(event_name) AS event_history ,
  collect_set(items.item_id),
  flatten(collect_set(items.item_id)),
  array_distinct(flatten(collect_set(items.item_id))) as cart_history
FROM events
GROUP BY user_id
----

* set operators:
** UNION
+
[source]
----
SELECT * FROM events 
UNION 
SELECT * FROM new_events_final
----

** INTERSECT
** MINUS

* Higher Order Functions
** FILTER: `FILTER (items, i -> i.item_id LIKE "%K") AS king_items`
+
[source]
----
CREATE OR REPLACE TEMP VIEW king_size_sales AS

SELECT order_id, king_items
FROM (
  SELECT
    order_id,
    FILTER (items, i -> i.item_id LIKE "%K") AS king_items
  FROM sales)
WHERE size(king_items) > 0;
  
SELECT * FROM king_size_sales
----

** TRANSFORM: `TRANSFORM(king_items, k -> CAST(k.item_revenue_in_usd * 100 AS INT)) AS item_revenues`
+
[source]
----
CREATE OR REPLACE TEMP VIEW king_item_revenues AS

SELECT
  order_id,
  king_items,
  TRANSFORM (
    king_items,
    k -> CAST(k.item_revenue_in_usd * 100 AS INT)
  ) AS item_revenues
FROM king_size_sales;

SELECT * FROM king_item_revenues
----

** REDUCE

=== PIVOT

Original table as

[source]
----
WITH my_sales AS (
  SELECT
      email,
      order_id,
      item.item_id AS item_id,
      item.quantity AS quantity
  FROM sales_enriched
) 

SELECT * FROM my_sales
----

|===
|email |order_id |item_id |quantity

|cassie17@medina-anderson.com
|445523
|M_PREM_K
|1

|cassie17@medina-anderson.com
|445523
|P_FOAM_K
|1

|=== 

with image:https://i.giphy.com/media/GZLf5Njk1KGwU/giphy.webp[PIVOT!!!] is turned into table as

[source]
----
WITH my_sales AS (
  SELECT
      email,
      order_id,
      item.item_id AS item_id,
      item.quantity AS quantity
  FROM sales_enriched
) 

SELECT * 
FROM my_sales
PIVOT (
  sum(quantity) FOR item_id in (
    'M_PREM_K',
    'P_FOAM_K',
    'P_FOAM_S'
  )
)
----

|=== 
|email |order_id |M_PREM_K |P_FOAM_K |P_FOAM_S

|cassie17@medina-anderson.com
|445523
|1
|NULL
|1

|=== 


== User Defined Functions (UDF)

[source]
----
CREATE OR REPLACE TEMPORARY VIEW foods(food) AS VALUES
("beef"),
("beans"),
("potatoes"),
("bread");

SELECT * FROM foods
----

[source]
----
CREATE OR REPLACE FUNCTION yelling(text STRING)
RETURNS STRING
RETURN concat(upper(text), "!!!")
----

[source]
----
SELECT yelling(food) FROM foods
----

returns

|===

|BEEF!!!
|BEANS!!!
|POTATOES!!!
|BREAD!!!

|===

=== CASE / WHEN

[source]
----
SELECT *,
  CASE 
    WHEN food = "beans" THEN "I love beans"
    WHEN food = "potatoes" THEN "My favorite vegetable is potatoes"
    WHEN food <> "beef" THEN concat("Do you have any good recipes for ", food ,"?")
    ELSE concat("I don't eat ", food)
  END
FROM foods
----

=== CASE / WHEN in UDF

[source]
----
CREATE FUNCTION foods_i_like(food STRING)
RETURNS STRING
RETURN CASE 
  WHEN food = "beans" THEN "I love beans"
  WHEN food = "potatoes" THEN "My favorite vegetable is potatoes"
  WHEN food <> "beef" THEN concat("Do you have any good recipes for ", food ,"?")
  ELSE concat("I don't eat ", food)
END;
----
